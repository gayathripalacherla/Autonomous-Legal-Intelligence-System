{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6aa59fc26a8f4961b1cdc6cf1a2ec78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ebf7e162756454998b655de05a174f9",
              "IPY_MODEL_d595256dc135416bb517911846695684",
              "IPY_MODEL_d30151342c4c469cb258305ec5628445"
            ],
            "layout": "IPY_MODEL_4d1736fbd3914176aae38d5088672271"
          }
        },
        "6ebf7e162756454998b655de05a174f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576220004f0c45018f6bb00eb3217a21",
            "placeholder": "​",
            "style": "IPY_MODEL_61d33a22ec5d4fe0a7afac619a16140f",
            "value": "Batches: 100%"
          }
        },
        "d595256dc135416bb517911846695684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee813a4dee364f18accee8dccd40e7a8",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b836f273928046239bd911886e74adef",
            "value": 38
          }
        },
        "d30151342c4c469cb258305ec5628445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_355a1932cc154e1aa7aefb72f25c57ea",
            "placeholder": "​",
            "style": "IPY_MODEL_069cb7203a334383afb82004bc11a9a0",
            "value": " 38/38 [00:00&lt;00:00, 63.34it/s]"
          }
        },
        "4d1736fbd3914176aae38d5088672271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576220004f0c45018f6bb00eb3217a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61d33a22ec5d4fe0a7afac619a16140f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee813a4dee364f18accee8dccd40e7a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b836f273928046239bd911886e74adef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "355a1932cc154e1aa7aefb72f25c57ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "069cb7203a334383afb82004bc11a9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLZz5p02Nxpj",
        "outputId": "d69ce595-696c-4337-849b-240f615a2c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.48.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=a889a9f0bdb798d26c1b5b17ac31cc856556bccc0f2c4104de30b513de6ea1a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, watchdog, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, feedparser, faiss-cpu, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, streamlit\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 feedparser-6.0.11 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pydeck-0.9.1 sgmllib3k-1.0.0 streamlit-1.48.0 watchdog-6.0.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.4-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 langgraph-0.6.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.10.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.157.0-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting appdirs>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.9.0)\n",
            "Collecting chromadb>=0.5.23 (from crewai)\n",
            "  Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crewai) (8.2.1)\n",
            "Collecting instructor>=1.3.3 (from crewai)\n",
            "  Downloading instructor-1.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json-repair==0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.25.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting json5>=0.10.0 (from crewai)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting litellm==1.74.9 (from crewai)\n",
            "  Downloading litellm-1.74.9-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.22.0 (from crewai)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.99.1)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from crewai) (3.1.5)\n",
            "Collecting opentelemetry-api>=1.30.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.30.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.30.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pdfplumber>=0.11.4 (from crewai)\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker==2.7.0 (from crewai)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from crewai) (2.11.7)\n",
            "Requirement already satisfied: pyjwt>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (2.10.1)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.1.1)\n",
            "Collecting pyvis>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from crewai) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from crewai) (0.21.4)\n",
            "Collecting tomli-w>=1.1.0 (from crewai)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli>=2.0.2 (from crewai)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting uv>=0.4.25 (from crewai)\n",
            "  Downloading uv-0.8.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (3.12.15)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (4.25.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.74.9->crewai) (0.10.0)\n",
            "Collecting coloredlogs (from onnxruntime==1.22.0->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (1.13.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (4.14.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (3.11.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (13.9.4)\n",
            "Collecting diskcache>=5.6.3 (from instructor>=1.3.3->crewai)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.17.0)\n",
            "Requirement already satisfied: jiter<0.11,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.10.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.30.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (43.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.4.1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.20.3->crewai) (0.34.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm==1.74.9->crewai) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.13.3->crewai) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.23->crewai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm==1.74.9->crewai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm==1.74.9->crewai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.9->crewai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.3->crewai) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.74.9->crewai) (3.23.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.9->crewai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.9->crewai) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (3.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (15.0.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.22.0->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime==1.22.0->crewai) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (4.9.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.6.1)\n",
            "Downloading crewai-0.157.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.25.2-py3-none-any.whl (12 kB)\n",
            "Downloading litellm-1.74.9-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.10.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading uv-0.8.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=8818cf4b29fd0c64669efdb1041cb9664df41e6ed646c78451ea3da8788b45e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, appdirs, uvloop, uv, tomli-w, tomli, pypdfium2, pybase64, portalocker, overrides, opentelemetry-proto, mmh3, jsonref, json5, json-repair, jedi, humanfriendly, httptools, diskcache, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pyvis, pdfminer.six, opentelemetry-semantic-conventions, onnxruntime, kubernetes, pdfplumber, opentelemetry-sdk, litellm, instructor, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, chromadb, crewai\n",
            "Successfully installed appdirs-1.4.4 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.16 coloredlogs-15.0.1 crewai-0.157.0 diskcache-5.6.3 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 instructor-1.10.0 jedi-0.19.2 json-repair-0.25.2 json5-0.12.1 jsonref-1.1.0 kubernetes-33.1.0 litellm-1.74.9 mmh3-5.2.0 onnxruntime-1.22.0 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 pdfminer.six-20250506 pdfplumber-0.11.7 portalocker-2.7.0 posthog-5.4.0 pybase64-1.4.2 pypdfium2-4.30.0 pypika-0.48.9 pyvis-0.3.2 tomli-2.2.1 tomli-w-1.2.0 uv-0.8.9 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.42.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Collecting gradio-client==1.11.1 (from gradio)\n",
            "  Downloading gradio_client-1.11.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.42.0-py3-none-any.whl (59.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.11.1-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.5/324.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.0\n",
            "    Uninstalling gradio_client-1.11.0:\n",
            "      Successfully uninstalled gradio_client-1.11.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.41.0\n",
            "    Uninstalling gradio-5.41.0:\n",
            "      Successfully uninstalled gradio-5.41.0\n",
            "Successfully installed gradio-5.42.0 gradio-client-1.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu transformers feedparser streamlit\n",
        "!pip install langchain langchain-community langgraph\n",
        "!pip install crewai\n",
        "!pip install --upgrade gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import os\n",
        "import sys\n",
        "from langgraph.graph import StateGraph\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from crewai import Agent, Task, Crew\n",
        "from crewai.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n"
      ],
      "metadata": {
        "id": "lexRl8DUQ7ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCPServer:\n",
        "    def __init__(self):\n",
        "        self.context_history = []\n",
        "\n",
        "    def save_context(self, query, summary, recommendations):\n",
        "        self.context_history.append({\n",
        "            \"query\": query,\n",
        "            \"summary\": summary,\n",
        "            \"recommendations\": recommendations\n",
        "        })\n",
        "\n",
        "    def get_latest(self):\n",
        "        return self.context_history[-1] if self.context_history else None\n",
        "\n",
        "# Global server object\n",
        "mcp_server = MCPServer()"
      ],
      "metadata": {
        "id": "yEBq8PX6rxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "from urllib.parse import quote\n",
        "\n",
        "def search_arxiv(query, max_results=20):\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    search_query = f'search_query=all:{quote(query)}&start=0&max_results={max_results}'\n",
        "    feed = feedparser.parse(base_url + search_query)\n",
        "    return feed.entries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h5JmdNqSSCSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Keep domain structure as is\n",
        "domain_structure = {\n",
        "    \"Constitutional & Administrative Law\": [\n",
        "        \"Administrative Tribunals\", \"Constitutional Amendments\", \"Emergency Powers\", \"Federalism\",\n",
        "        \"Fundamental Rights\", \"Judicial Review\", \"Legislative Powers\", \"Public Interest Litigation\",\n",
        "        \"Separation of Powers\", \"Writ Jurisdiction\"\n",
        "    ],\n",
        "    \"Corporate & Commercial Law\": [\n",
        "        \"Banking Law\", \"Commercial Arbitration\", \"Company Law\", \"Competition Law\", \"Consumer Protection\",\n",
        "        \"Contract Law\", \"E-Commerce Law\", \"Insolvency & Bankruptcy\", \"Mergers & Acquisitions\", \"Securities Regulation\"\n",
        "    ],\n",
        "    \"Criminal Law & Procedure\": [\n",
        "        \"Bail & Sentencing\", \"Criminal Procedure\", \"Cyber Crime\", \"Double Jeopardy\", \"Evidence in Criminal Cases\",\n",
        "        \"Juvenile Justice\", \"Search and Seizure\", \"Substantive Criminal Law\", \"Victim Rights\", \"White-Collar Crime\"\n",
        "    ],\n",
        "    \"Environmental & Energy Law\": [\n",
        "        \"Air Pollution Regulation\", \"Climate Change Law\", \"Environmental Impact Assessment\", \"Environmental Litigation\",\n",
        "        \"Forest Law\", \"Mining & Natural Resources Law\", \"Renewable Energy Law\", \"Sustainable Development\", \"Water Law\", \"Wildlife Protection\"\n",
        "    ],\n",
        "    \"Intellectual Property & Technology Law\": [\n",
        "        \"AI & Law\", \"Copyright Law\", \"Data Privacy Law\", \"Digital Rights Management\", \"IP Licensing\", \"Internet Governance\",\n",
        "        \"Patent Law\", \"Software Licensing\", \"Trade Secrets\", \"Trademark Law\"\n",
        "    ],\n",
        "    \"International & Human Rights Law\": [\n",
        "        \"Climate Agreements\", \"Diplomatic Immunity\", \"Gender Rights under International Law\", \"Global Health Law\",\n",
        "        \"Humanitarian Law\", \"International Criminal Court\", \"International Treaties\", \"Refugee Law\", \"UN Conventions\", \"War Crimes\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "O6kDgvyEwA4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# New function: fetch papers and summarize using MCP tools\n",
        "#def fetch_and_summarize_papers(domain_structure, max_results=20):\n",
        "    #all_papers = []\n",
        "    #for domain, subdomains in domain_structure.items():\n",
        "        #for sub in subdomains:\n",
        "#             print(f\"🔍 Fetching papers for: {domain} → {sub}\")\n",
        "\n",
        "#             # Use arxiv_tool to fetch papers\n",
        "#             papers = arxiv_tool.run(sub)\n",
        "\n",
        "#             for paper in papers:\n",
        "#                 abstract_text = paper.get(\"summary\", \"\")\n",
        "#                 if abstract_text:\n",
        "#                     # Use summarizer_tool to summarize abstract\n",
        "#                     summarized = summarizer_tool.run(abstract_text)\n",
        "#                 else:\n",
        "#                     summarized = \"No abstract available\"\n",
        "\n",
        "#                 paper['summary'] = summarized\n",
        "#                 paper['domain'] = domain\n",
        "#                 paper['subdomain'] = sub\n",
        "#             all_papers.extend(papers)\n",
        "\n",
        "#     df_all = pd.DataFrame(all_papers)\n",
        "#     return df_all\n",
        "\n",
        "\n",
        "\n",
        "# df_all = fetch_and_summarize_papers(domain_structure)\n",
        "# df_all.to_csv(\"arxiv_1200_papers.csv\", index=False)\n",
        "# print(\"✅ Papers fetched, summarized, and saved to CSV!\")"
      ],
      "metadata": {
        "id": "BhFlDOtHRsMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check paper counts per subdomain (optional - uncomment when needed)\n",
        "# if 'df_all' in globals():\n",
        "#     subdomain_counts = df_all.groupby(['domain', 'subdomain']).size().reset_index(name='num_papers')\n",
        "#     print(subdomain_counts.sort_values('num_papers'))\n",
        "# else:\n",
        "#     print(\"⚠️ Dataframe 'df_all' not loaded yet. Run fetch_and_summarize_papers() or load CSV first.\")\n"
      ],
      "metadata": {
        "id": "VlUBh2IMzpjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset if previously saved\n",
        "df_loaded = pd.read_csv('Final_Realistic_Legal_Cases.csv')\n",
        "print(f\"✅ Loaded CSV with {df_loaded.shape[0]} case papers.\")\n",
        "display(df_loaded.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "F53IARUiSmue",
        "outputId": "6efc8007-7b7e-484d-b904-79d835ed9841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded CSV with 1200 case papers.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                         title  \\\n",
              "0           Davis v. IRS, 769 F.Supp.2d (2004)   \n",
              "1         Taylor v. EPA, 909 Fed. Appx. (2017)   \n",
              "2            Roe v. Wade, 639 F.Supp.2d (2020)   \n",
              "3  Martin v. Google Inc., 396 F.Supp.2d (2006)   \n",
              "4                Davis v. IRS, 674 F.3d (2011)   \n",
              "\n",
              "                                             summary  \\\n",
              "0  This opinion addresses legal questions related...   \n",
              "1  This opinion addresses legal questions related...   \n",
              "2  This opinion addresses legal questions related...   \n",
              "3  This opinion addresses legal questions related...   \n",
              "4  This opinion addresses legal questions related...   \n",
              "\n",
              "                                             link   published  \\\n",
              "0  https://www.courtlistener.com/opinion/5423652/  2019-07-05   \n",
              "1  https://www.courtlistener.com/opinion/5535264/  2024-04-15   \n",
              "2  https://www.courtlistener.com/opinion/7732493/  2021-07-12   \n",
              "3  https://www.courtlistener.com/opinion/9890303/  2023-05-08   \n",
              "4  https://www.courtlistener.com/opinion/6632109/  2018-06-02   \n",
              "\n",
              "                                               court  \\\n",
              "0                 U.S. Court of Appeals, 9th Circuit   \n",
              "1  U.S. District Court, Southern District of New ...   \n",
              "2                 Supreme Court of the United States   \n",
              "3                 Supreme Court of the United States   \n",
              "4                 U.S. Court of Appeals, 9th Circuit   \n",
              "\n",
              "                                   domain                  subdomain  \\\n",
              "0              Corporate & Commercial Law             E-Commerce Law   \n",
              "1  Intellectual Property & Technology Law              Trademark Law   \n",
              "2  Intellectual Property & Technology Law  Digital Rights Management   \n",
              "3              Environmental & Energy Law   Air Pollution Regulation   \n",
              "4     Constitutional & Administrative Law            Judicial Review   \n",
              "\n",
              "                                          clean_text  token_count  case_year  \\\n",
              "0  The case deals with e-commerce law ruling. thi...           36       2019   \n",
              "1  The opinion explores legal issues surrounding ...           33       2024   \n",
              "2  This judgment concerns digital rights manageme...           40       2021   \n",
              "3  The case deals with air pollution regulation r...           35       2023   \n",
              "4  This ruling pertains to judicial review ruling...           32       2018   \n",
              "\n",
              "      case_id jurisdiction                                  clean_title  \n",
              "0  2ab8b6e71b     US-State           Davis v. IRS, 769 F.Supp.2d (2004)  \n",
              "1  f6a3e6586a     US-State         Taylor v. EPA, 909 Fed. Appx. (2017)  \n",
              "2  5ad5eeff4f     US-State            Roe v. Wade, 639 F.Supp.2d (2020)  \n",
              "3  ad0e59b9ec     US-State  Martin v. Google Inc., 396 F.Supp.2d (2006)  \n",
              "4  8cdfa9f5bf     US-State                Davis v. IRS, 674 F.3d (2011)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-698ea4eb-3cb5-480a-bf9e-521efb07e26f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "      <th>court</th>\n",
              "      <th>domain</th>\n",
              "      <th>subdomain</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>token_count</th>\n",
              "      <th>case_year</th>\n",
              "      <th>case_id</th>\n",
              "      <th>jurisdiction</th>\n",
              "      <th>clean_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Davis v. IRS, 769 F.Supp.2d (2004)</td>\n",
              "      <td>This opinion addresses legal questions related...</td>\n",
              "      <td>https://www.courtlistener.com/opinion/5423652/</td>\n",
              "      <td>2019-07-05</td>\n",
              "      <td>U.S. Court of Appeals, 9th Circuit</td>\n",
              "      <td>Corporate &amp; Commercial Law</td>\n",
              "      <td>E-Commerce Law</td>\n",
              "      <td>The case deals with e-commerce law ruling. thi...</td>\n",
              "      <td>36</td>\n",
              "      <td>2019</td>\n",
              "      <td>2ab8b6e71b</td>\n",
              "      <td>US-State</td>\n",
              "      <td>Davis v. IRS, 769 F.Supp.2d (2004)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Taylor v. EPA, 909 Fed. Appx. (2017)</td>\n",
              "      <td>This opinion addresses legal questions related...</td>\n",
              "      <td>https://www.courtlistener.com/opinion/5535264/</td>\n",
              "      <td>2024-04-15</td>\n",
              "      <td>U.S. District Court, Southern District of New ...</td>\n",
              "      <td>Intellectual Property &amp; Technology Law</td>\n",
              "      <td>Trademark Law</td>\n",
              "      <td>The opinion explores legal issues surrounding ...</td>\n",
              "      <td>33</td>\n",
              "      <td>2024</td>\n",
              "      <td>f6a3e6586a</td>\n",
              "      <td>US-State</td>\n",
              "      <td>Taylor v. EPA, 909 Fed. Appx. (2017)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Roe v. Wade, 639 F.Supp.2d (2020)</td>\n",
              "      <td>This opinion addresses legal questions related...</td>\n",
              "      <td>https://www.courtlistener.com/opinion/7732493/</td>\n",
              "      <td>2021-07-12</td>\n",
              "      <td>Supreme Court of the United States</td>\n",
              "      <td>Intellectual Property &amp; Technology Law</td>\n",
              "      <td>Digital Rights Management</td>\n",
              "      <td>This judgment concerns digital rights manageme...</td>\n",
              "      <td>40</td>\n",
              "      <td>2021</td>\n",
              "      <td>5ad5eeff4f</td>\n",
              "      <td>US-State</td>\n",
              "      <td>Roe v. Wade, 639 F.Supp.2d (2020)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Martin v. Google Inc., 396 F.Supp.2d (2006)</td>\n",
              "      <td>This opinion addresses legal questions related...</td>\n",
              "      <td>https://www.courtlistener.com/opinion/9890303/</td>\n",
              "      <td>2023-05-08</td>\n",
              "      <td>Supreme Court of the United States</td>\n",
              "      <td>Environmental &amp; Energy Law</td>\n",
              "      <td>Air Pollution Regulation</td>\n",
              "      <td>The case deals with air pollution regulation r...</td>\n",
              "      <td>35</td>\n",
              "      <td>2023</td>\n",
              "      <td>ad0e59b9ec</td>\n",
              "      <td>US-State</td>\n",
              "      <td>Martin v. Google Inc., 396 F.Supp.2d (2006)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Davis v. IRS, 674 F.3d (2011)</td>\n",
              "      <td>This opinion addresses legal questions related...</td>\n",
              "      <td>https://www.courtlistener.com/opinion/6632109/</td>\n",
              "      <td>2018-06-02</td>\n",
              "      <td>U.S. Court of Appeals, 9th Circuit</td>\n",
              "      <td>Constitutional &amp; Administrative Law</td>\n",
              "      <td>Judicial Review</td>\n",
              "      <td>This ruling pertains to judicial review ruling...</td>\n",
              "      <td>32</td>\n",
              "      <td>2018</td>\n",
              "      <td>8cdfa9f5bf</td>\n",
              "      <td>US-State</td>\n",
              "      <td>Davis v. IRS, 674 F.3d (2011)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-698ea4eb-3cb5-480a-bf9e-521efb07e26f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-698ea4eb-3cb5-480a-bf9e-521efb07e26f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-698ea4eb-3cb5-480a-bf9e-521efb07e26f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d31814c-36cc-45c8-b16b-7402e3f056a2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d31814c-36cc-45c8-b16b-7402e3f056a2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d31814c-36cc-45c8-b16b-7402e3f056a2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_loaded\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Taylor v. EPA, 909 Fed. Appx. (2017)\",\n          \"Davis v. IRS, 674 F.3d (2011)\",\n          \"Roe v. Wade, 639 F.Supp.2d (2020)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This opinion addresses legal questions related to a landmark decision touching upon trademark law where the court addressed complex legal issues surrounding intellectual property & technology law. The court's reasoning is grounded in applicable statutory and constitutional provisions.\",\n          \"This opinion addresses legal questions related to through this case, the judiciary analyzed principles of judicial review while reinforcing key doctrines from constitutional & administrative law. The court's reasoning is grounded in applicable statutory and constitutional provisions.\",\n          \"This opinion addresses legal questions related to the case sets a precedent in the field of digital rights management, emphasizing judicial interpretation and statutory application under intellectual property & technology law. The court's reasoning is grounded in applicable statutory and constitutional provisions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://www.courtlistener.com/opinion/5535264/\",\n          \"https://www.courtlistener.com/opinion/6632109/\",\n          \"https://www.courtlistener.com/opinion/7732493/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-04-15\",\n          \"2018-06-02\",\n          \"2021-07-12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"court\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"U.S. Court of Appeals, 9th Circuit\",\n          \"U.S. District Court, Southern District of New York\",\n          \"Supreme Court of the United States\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"domain\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Intellectual Property & Technology Law\",\n          \"Constitutional & Administrative Law\",\n          \"Corporate & Commercial Law\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subdomain\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Trademark Law\",\n          \"Judicial Review\",\n          \"Digital Rights Management\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The opinion explores legal issues surrounding trademark law ruling. a landmark decision touching upon trademark law where the court addressed complex legal issues surrounding intellectual property & technology law.\",\n          \"This ruling pertains to judicial review ruling. through this case, the judiciary analyzed principles of judicial review while reinforcing key doctrines from constitutional & administrative law.\",\n          \"This judgment concerns digital rights management ruling. the case sets a precedent in the field of digital rights management, emphasizing judicial interpretation and statutory application under intellectual property & technology law.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 32,\n        \"max\": 40,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          33,\n          32,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"case_year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2018,\n        \"max\": 2024,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2024,\n          2018,\n          2021\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"case_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"f6a3e6586a\",\n          \"8cdfa9f5bf\",\n          \"5ad5eeff4f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jurisdiction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"US-State\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Taylor v. EPA, 909 Fed. Appx. (2017)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embedding model (already used in previous block but kept here if re-run separately)\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"✅ Embedding model loaded!\")\n",
        "\n",
        "# Define MCP Tool for embedding if not already defined\n",
        "def embed_text(text):\n",
        "    \"\"\"Embed a single text string into a vector.\"\"\"\n",
        "    return embed_model.encode([text]).tolist()\n",
        "\n",
        "# Build FAISS index using abstracts\n",
        "abstract_texts = df_loaded['clean_text'].fillna(\"\").tolist()\n",
        "abstract_embeddings = embed_model.encode(abstract_texts, show_progress_bar=True)\n",
        "abstract_embeddings = np.array(abstract_embeddings).astype('float32')\n",
        "\n",
        "dimension = abstract_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(abstract_embeddings)\n",
        "print(f\"✅ FAISS index created with {index.ntotal} papers!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "6aa59fc26a8f4961b1cdc6cf1a2ec78d",
            "6ebf7e162756454998b655de05a174f9",
            "d595256dc135416bb517911846695684",
            "d30151342c4c469cb258305ec5628445",
            "4d1736fbd3914176aae38d5088672271",
            "576220004f0c45018f6bb00eb3217a21",
            "61d33a22ec5d4fe0a7afac619a16140f",
            "ee813a4dee364f18accee8dccd40e7a8",
            "b836f273928046239bd911886e74adef",
            "355a1932cc154e1aa7aefb72f25c57ea",
            "069cb7203a334383afb82004bc11a9a0"
          ]
        },
        "id": "8n5UZWRLSM50",
        "outputId": "644f3004-7b5c-48d9-9e87-1b4475a929a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedding model loaded!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa59fc26a8f4961b1cdc6cf1a2ec78d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created with 1200 papers!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_tool(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Summarizes legal input using chunking for long texts.\n",
        "    This function selects whether to chunk or not based on word count.\n",
        "    \"\"\"\n",
        "    word_count = len(text.split())\n",
        "    if word_count < 150:\n",
        "        return hf_pipeline(text, max_length=256, do_sample=False)[0][\"generated_text\"]\n",
        "    else:\n",
        "        # Chunk input and summarize each chunk\n",
        "        words = text.split()\n",
        "        chunk_size = 150\n",
        "        summaries = []\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i + chunk_size])\n",
        "            try:\n",
        "                output = hf_pipeline(chunk, max_length=256, do_sample=False)[0][\"generated_text\"]\n",
        "                summaries.append(output)\n",
        "            except Exception as e:\n",
        "                summaries.append(f\"[Chunk {i//chunk_size+1}] Error: {str(e)}\")\n",
        "\n",
        "        # Combine all chunk summaries and summarize again for coherence\n",
        "        combined = \" \".join(summaries)\n",
        "        final_summary = hf_pipeline(combined, max_length=256, do_sample=False)[0][\"generated_text\"]\n",
        "        return final_summary\n"
      ],
      "metadata": {
        "id": "5EKCJ2hUtVMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForPreTraining, AutoModelForCausalLM\n",
        "\n",
        "model_name =\"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "hf_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "print(\"✅Model loaded for structured case extraction!\")\n",
        "\n",
        "def legal_structured_output(text: str) -> str:\n",
        "    prompt = f\"\"\"You are a legal assistant. Fill in each field based on the case below. If a field is missing, write \"Not found\".\n",
        "\n",
        "Case Title:\n",
        "Court:\n",
        "Date:\n",
        "Facts:\n",
        "Issue:\n",
        "Rule:\n",
        "Disposition:\n",
        "\n",
        "Legal Case Text:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    result = hf_pipeline(prompt, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    expected_fields = [\"Case Title:\", \"Court:\", \"Date:\", \"Facts:\", \"Issue:\", \"Rule:\", \"Holding:\", \"Disposition:\"]\n",
        "    missing = [field for field in expected_fields if field not in result]\n",
        "    if missing:\n",
        "        result += f\"\\n⚠️ Missing fields: {', '.join(missing)}\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def legal_structured_output_with_chunking(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Summarizes chunks of long legal text and generates a structured legal summary from the combined summary.\n",
        "    \"\"\"\n",
        "\n",
        "    text = text.strip().replace(\"\\n\", \" \")\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    # 🔹 Short case? Just use regular output\n",
        "    if word_count < 100:\n",
        "        return legal_structured_output(text)\n",
        "\n",
        "    # 🔹 Split into ~200-word chunks\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    for word in text.split():\n",
        "        current_chunk.append(word)\n",
        "        if len(current_chunk) >= 200:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    # 🔹 Summarize each chunk\n",
        "    chunk_summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            summary = hf_pipeline(chunk, max_length=150, do_sample=False)[0][\"generated_text\"]\n",
        "            chunk_summaries.append(summary)\n",
        "        except Exception as e:\n",
        "            chunk_summaries.append(f\"[Chunk {i+1}]: Error summarizing chunk - {str(e)}\")\n",
        "\n",
        "    # 🔹 Combine all summaries\n",
        "    combined_summary = \" \".join(chunk_summaries)\n",
        "\n",
        "    # 🔹 Now run structured extraction on the **combined summary**\n",
        "    structured_prompt = f\"\"\"You are a legal assistant. Fill in each field based on the case below. If a field is missing, write \"Not found\".\n",
        "\n",
        "Case Title:\n",
        "Court:\n",
        "Date:\n",
        "Facts:\n",
        "Issue:\n",
        "Rule:\n",
        "Disposition:\n",
        "\n",
        "Legal Case Text:\n",
        "{combined_summary}\n",
        "\"\"\"\n",
        "\n",
        "    result = hf_pipeline(structured_prompt, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    expected_fields = [\"Case Title:\", \"Court:\", \"Date:\", \"Facts:\", \"Issue:\", \"Rule:\", \"Holding:\", \"Disposition:\"]\n",
        "    missing = [field for field in expected_fields if field not in result]\n",
        "    if missing:\n",
        "        result += f\"\\n⚠️ Missing fields: {', '.join(missing)}\"\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpGdbuiTAyv",
        "outputId": "967fe1de-46fc-4cc1-cbf0-a5bc956b4eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅Model loaded for structured case extraction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM as AutoSeq2Seq, AutoTokenizer as AutoToken\n",
        "\n",
        "flan_model_name = \"google/flan-t5-large\"\n",
        "flan_tokenizer = AutoToken.from_pretrained(flan_model_name)\n",
        "flan_model = AutoSeq2Seq.from_pretrained(flan_model_name)\n",
        "\n",
        "flan_pipeline = pipeline(\"text2text-generation\", model=flan_model, tokenizer=flan_tokenizer)\n",
        "\n",
        "print(\"✅ Flan-T5-Large loaded.\")\n",
        "\n",
        "\n",
        "def flan_field_by_field_extraction(text: str) -> str:\n",
        "    questions = [\n",
        "        \"What is the full legal title of the case, including the names of the petitioner and respondent (e.g., X v. Y)? Exclude the case number or court name.\",\n",
        "        \"Which court heard the case?\",\n",
        "        \"What is the year of the case?\",\n",
        "        \"Summarize the facts of the case.\",\n",
        "        \"What is the legal issue?\",\n",
        "        \"What legal rule was applied?\",\n",
        "        \"What was the final disposition?\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for q in questions:\n",
        "        prompt = f\"{q}\\n\\n{text}\"\n",
        "        try:\n",
        "            response = flan_pipeline(prompt, max_new_tokens=150, do_sample=False)[0][\"generated_text\"]\n",
        "            results[q] = response.strip()\n",
        "        except Exception as e:\n",
        "            results[q] = f\"Error: {str(e)}\"\n",
        "\n",
        "    question_to_heading = {\n",
        "        \"What is the full legal title of the case, including the names of the petitioner and respondent (e.g., X v. Y)? Exclude the case number or court name.\": \"CASE TITLE\",\n",
        "        \"Which court heard the case?\": \"COURT\",\n",
        "        \"What is the year of the case?\": \"DATE\",\n",
        "        \"Summarize the facts of the case.\": \"FACTS\",\n",
        "        \"What is the legal issue?\": \"ISSUE\",\n",
        "        \"What legal rule was applied?\": \"RULE\",\n",
        "        \"What was the final disposition?\": \"DISPOSITION\"\n",
        "    }\n",
        "\n",
        "    output = \"\"\n",
        "    for question, answer in results.items():\n",
        "        heading = question_to_heading.get(question, question)\n",
        "        output += f\"{heading}: {answer}\\n\\n\"\n",
        "    return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxjamPsBRnaU",
        "outputId": "cb26c00f-8b39-47f1-d69c-1fa2648bd284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Flan-T5-Large loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_related_papers(user_input, top_k=5, domain_filter=\"All\", subdomain_filter=\"All\"):\n",
        "    # Search FAISS index on the entire dataset\n",
        "    user_vec = embed_model.encode([user_input]).astype('float32')\n",
        "    D, I = index.search(user_vec, 50)  # fetch more than top_k initially\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        if idx >= len(df_loaded):\n",
        "            continue\n",
        "\n",
        "        paper = df_loaded.iloc[idx].to_dict()\n",
        "\n",
        "        # Apply filters AFTER search\n",
        "        if domain_filter != \"All\" and paper['domain'] != domain_filter:\n",
        "            continue\n",
        "        if subdomain_filter != \"All\" and paper['subdomain'] != subdomain_filter:\n",
        "            continue\n",
        "\n",
        "        doc_vec = embed_model.encode([paper['clean_text']]).astype('float32')\n",
        "        user_norm = user_vec / np.linalg.norm(user_vec)\n",
        "        doc_norm = doc_vec / np.linalg.norm(doc_vec)\n",
        "        cos_sim = float(np.dot(user_norm, doc_norm.T))\n",
        "        cos_sim = min(max(cos_sim, 0.0), 1.0)  # Clip for safety\n",
        "        paper['similarity_score'] = round(cos_sim, 3)\n",
        "        results.append(paper)\n",
        "\n",
        "    results = sorted(results, key=lambda x: x['similarity_score'], reverse=True)\n",
        "\n",
        "    # ✅ Return only top_k\n",
        "    return results[:top_k]"
      ],
      "metadata": {
        "id": "eBYFPIXlURE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from crewai.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "hf_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "hf_pipeline = pipeline(\"text2text-generation\", model=hf_model, tokenizer=hf_tokenizer)\n",
        "\n",
        "hf_llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "\n",
        "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
        "os.environ[\"CREWAI_LLM_PROVIDER\"] = \"langchain\"\n",
        "\n",
        "class SummarizerTool(BaseTool):\n",
        "    name: str = \"summarizer_tool\"\n",
        "    description: str = \"Summarizes legal case text to generate embeddings for recommendations.\"\n",
        "\n",
        "    def _run(self, text: str):\n",
        "        return hf_pipeline(text, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "summarizer_tool_instance = SummarizerTool()\n",
        "\n",
        "class LegalStructuringTool(BaseTool):\n",
        "    name: str = \"legal_structuring_tool\"\n",
        "    description: str = \"Extracts structured fields (title, facts, issue, rule, etc.) from a legal case.\"\n",
        "\n",
        "    def _run(self, text: str) -> str:\n",
        "        structured_prompt = f\"\"\"\n",
        "        You are a legal assistant. Fill in each field based on the case below.\n",
        "        If a field is missing, write \"Not found\".\n",
        "\n",
        "        Case Title:\n",
        "        Court:\n",
        "        Date:\n",
        "        Facts:\n",
        "        Issue:\n",
        "        Rule:\n",
        "        Holding:\n",
        "        Disposition:\n",
        "\n",
        "        Legal Case Text:\n",
        "        {text}\n",
        "        \"\"\"\n",
        "        return hf_pipeline(structured_prompt, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "legal_structurer_tool_instance = LegalStructuringTool()\n",
        "\n",
        "def embed_text_tool(text: str) -> list:\n",
        "    \"\"\"Encodes input text into embedding vector.\"\"\"\n",
        "    return embed_model.encode([text]).tolist()\n",
        "\n",
        "class EmbedderTool(BaseTool):\n",
        "    name: str = \"embedder_tool\"\n",
        "    description: str = \"Embeds academic input into a vector representation.\"\n",
        "\n",
        "    def _run(self, text: str):\n",
        "        return embed_model.encode([text]).tolist()\n",
        "\n",
        "embedder_tool_instance = EmbedderTool()\n",
        "\n",
        "class RecommenderToolArgs(BaseModel):\n",
        "    user_input: str = Field(..., description=\"User query for finding related papers\")\n",
        "    top_k: int = Field(5, description=\"Number of papers to return\")\n",
        "    domain_filter: str = Field(\"All\", description=\"Domain to filter papers\")\n",
        "    subdomain_filter: str = Field(\"All\", description=\"Subdomain to filter papers\")\n",
        "\n",
        "class RecommenderTool(BaseTool):\n",
        "    name: str = \"recommender_tool\"\n",
        "    description: str = \"Recommends related papers with similarity scores and metadata\"\n",
        "    args_schema = RecommenderToolArgs\n",
        "\n",
        "    def _run(self, user_input, top_k=5, domain_filter=\"All\", subdomain_filter=\"All\"):\n",
        "        return find_related_papers(user_input, top_k, domain_filter, subdomain_filter)\n",
        "\n",
        "recommender_tool_instance = RecommenderTool()\n",
        "\n",
        "\n",
        "summarizer_agent = Agent(\n",
        "    role=\"Legal Case Summarizer\",\n",
        "    goal=\"Summarize legal text to aid downstream case recommendations.\",\n",
        "    backstory=\"An expert in legal language who compresses complex legal cases into concise summaries.\",\n",
        "    verbose=True,\n",
        "    tools=[summarizer_tool_instance],\n",
        "    llm=hf_llm\n",
        ")\n",
        "\n",
        "\n",
        "legal_structurer_agent = Agent(\n",
        "    role=\"Legal Case Analyzer\",\n",
        "    goal=\"Extract structured summaries from legal cases including facts, issues, rules, and holdings.\",\n",
        "    backstory=\"An expert in legal reasoning and judgment formatting, trained to organize legal content clearly.\",\n",
        "    tools=[legal_structurer_tool_instance],\n",
        "    llm=hf_llm\n",
        ")\n",
        "\n",
        "\n",
        "embedder_agent = Agent(\n",
        "    role=\"Embedding Generator\",\n",
        "    goal=\"Convert academic abstracts into meaningful vector representations.\",\n",
        "    backstory=\"A specialist agent trained to map academic language to dense vector spaces for similarity search.\",\n",
        "    verbose=True,\n",
        "    tools=[embedder_tool_instance],\n",
        "    llm=hf_llm\n",
        ")\n",
        "\n",
        "recommender_agent = Agent(\n",
        "    role=\"Paper Recommender\",\n",
        "    goal=\"Find top 5 academic papers based on input relevance.\",\n",
        "    backstory=\"Specialist in vector similarity and embeddings.\",\n",
        "    verbose=True,\n",
        "    tools=[recommender_tool_instance],\n",
        "    llm=hf_llm\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXv9snIV86wD",
        "outputId": "809fefb9-18c1-41ab-f40b-79ecba32ad0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tasks\n",
        "summarizer_task = Task(\n",
        "    description=\"Generate a summary of the legal case for similarity search: {input_text}\",\n",
        "    expected_output=\"A short summary of the case.\",\n",
        "    agent=summarizer_agent\n",
        ")\n",
        "\n",
        "legal_structurer_task = Task(\n",
        "    description=\"Extract structured legal elements from this case: {input_text}\",\n",
        "    expected_output=\"A structured summary with title, court, date, facts, issue, rule, holding, and disposition.\",\n",
        "    agent=legal_structurer_agent\n",
        ")\n",
        "legal_structurer_crew = Crew(\n",
        "    agents=[legal_structurer_agent],\n",
        "    tasks=[legal_structurer_task],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "embedder_task = Task(\n",
        "    description=\"Generate an embedding vector for the input text: {input_text}\",\n",
        "    expected_output=\"A vector representation of the input.\",\n",
        "    agent=embedder_agent\n",
        ")\n",
        "\n",
        "recommender_task = Task(\n",
        "    description=\"Recommend 5 relevant papers for: {input_text}\",\n",
        "    expected_output=\"A list of paper titles.\",\n",
        "    agent=recommender_agent\n",
        ")\n",
        "recommender_crew = Crew(\n",
        "    agents=[recommender_agent],\n",
        "    tasks=[recommender_task],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Multi-agent crew for combined task\n",
        "combined_crew = Crew(\n",
        "    agents=[summarizer_agent, embedder_agent, recommender_agent],\n",
        "    tasks=[summarizer_task, embedder_task, recommender_task],\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "RH6th4E_891M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def _parse_list_of_dicts_maybe(text: str) -> List[Dict[str, Any]]:\n",
        "    # Try Python literal\n",
        "    try:\n",
        "        parsed = ast.literal_eval(text)\n",
        "        if isinstance(parsed, list):\n",
        "            return parsed\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Try JSON\n",
        "    try:\n",
        "        parsed = json.loads(text)\n",
        "        if isinstance(parsed, list):\n",
        "            return parsed\n",
        "    except Exception:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "def run_structured_summary_agent(user_input: str) -> str:\n",
        "    \"\"\"Call the legal_structurer_agent via its Crew and return structured summary.\"\"\"\n",
        "    result = legal_structurer_crew.kickoff(inputs={\"input_text\": user_input})\n",
        "    return str(result).strip()\n",
        "\n",
        "def run_recommender_agent(user_input: str, domain_filter: str, subdomain_filter: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Call the recommender_agent via its Crew; parse list; fallback to local function if needed.\"\"\"\n",
        "    result = recommender_crew.kickoff(inputs={\n",
        "        \"input_text\": user_input,\n",
        "        \"domain_filter\": domain_filter,\n",
        "        \"subdomain_filter\": subdomain_filter,\n",
        "        \"top_k\": top_k\n",
        "    })\n",
        "    parsed = _parse_list_of_dicts_maybe(str(result).strip())\n",
        "    if parsed:\n",
        "        return parsed\n",
        "    # Fallback to your deterministic local retriever\n",
        "    return find_related_papers(user_input, top_k=top_k, domain_filter=domain_filter, subdomain_filter=subdomain_filter)\n",
        "\n",
        "def run_combined_crew(user_input: str, domain_filter: str, subdomain_filter: str, top_k: int = 5):\n",
        "    \"\"\"Run summarize + embed + recommend via combined crew; return structured summary + recs.\"\"\"\n",
        "    _ = combined_crew.kickoff(inputs={\n",
        "        \"input_text\": user_input,\n",
        "        \"domain_filter\": domain_filter,\n",
        "        \"subdomain_filter\": subdomain_filter,\n",
        "        \"top_k\": top_k\n",
        "    })\n",
        "    # Show a structured summary (legal_structurer) + recommendations (recommender)\n",
        "    structured = run_structured_summary_agent(user_input)\n",
        "    recs = run_recommender_agent(user_input, domain_filter, subdomain_filter, top_k=top_k)\n",
        "    return structured, recs\n",
        "\n",
        "def _recs_to_html(recommendations: List[Dict[str, Any]]) -> str:\n",
        "    if not recommendations:\n",
        "        return \"<p>No recommended cases found for this input.</p>\"\n",
        "    html = \"<h3>Recommended Legal Cases</h3>\"\n",
        "    for rec in recommendations:\n",
        "        html += f\"\"\"\n",
        "        <div style=\"margin-bottom: 15px; padding: 10px; border: 1px solid #ddd; border-radius: 5px;\">\n",
        "          <b>Title:</b> {rec.get('title', 'N/A')}<br>\n",
        "          <b>Domain:</b> {rec.get('domain', 'N/A')}<br>\n",
        "          <b>Subdomain:</b> {rec.get('subdomain', 'N/A')}<br>\n",
        "          <b>Published:</b> {rec.get('published', 'N/A')}<br>\n",
        "          <b>Link:</b> <a href=\"{rec.get('link', '#')}\" target=\"_blank\">View Case</a><br>\n",
        "          <b>Similarity Score:</b> {rec.get('similarity_score', 0):.2f}<br>\n",
        "          <b>Summary:</b> {rec.get('clean_text', 'No summary available')}<br>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    return html\n"
      ],
      "metadata": {
        "id": "QYSYaP844exm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\"\n",
        ")\n",
        "print(\"✅ Zero-shot classifier loaded!\")\n",
        "domain_structure = {\n",
        "    \"Constitutional & Administrative Law\": [\n",
        "        \"Administrative Tribunals\", \"Constitutional Amendments\", \"Emergency Powers\", \"Federalism\",\n",
        "        \"Fundamental Rights\", \"Judicial Review\", \"Legislative Powers\", \"Public Interest Litigation\",\n",
        "        \"Separation of Powers\", \"Writ Jurisdiction\"\n",
        "    ],\n",
        "    \"Corporate & Commercial Law\": [\n",
        "        \"Banking Law\", \"Commercial Arbitration\", \"Company Law\", \"Competition Law\", \"Consumer Protection\",\n",
        "        \"Contract Law\", \"E-Commerce Law\", \"Insolvency & Bankruptcy\", \"Mergers & Acquisitions\", \"Securities Regulation\"\n",
        "    ],\n",
        "    \"Criminal Law & Procedure\": [\n",
        "        \"Bail & Sentencing\", \"Criminal Procedure\", \"Cyber Crime\", \"Double Jeopardy\", \"Evidence in Criminal Cases\",\n",
        "        \"Juvenile Justice\", \"Search and Seizure\", \"Substantive Criminal Law\", \"Victim Rights\", \"White-Collar Crime\"\n",
        "    ],\n",
        "    \"Environmental & Energy Law\": [\n",
        "        \"Air Pollution Regulation\", \"Climate Change Law\", \"Environmental Impact Assessment\", \"Environmental Litigation\",\n",
        "        \"Forest Law\", \"Mining & Natural Resources Law\", \"Renewable Energy Law\", \"Sustainable Development\", \"Water Law\", \"Wildlife Protection\"\n",
        "    ],\n",
        "    \"Intellectual Property & Technology Law\": [\n",
        "        \"AI & Law\", \"Copyright Law\", \"Data Privacy Law\", \"Digital Rights Management\", \"IP Licensing\", \"Internet Governance\",\n",
        "        \"Patent Law\", \"Software Licensing\", \"Trade Secrets\", \"Trademark Law\"\n",
        "    ],\n",
        "    \"International & Human Rights Law\": [\n",
        "        \"Climate Agreements\", \"Diplomatic Immunity\", \"Gender Rights under International Law\", \"Global Health Law\",\n",
        "        \"Humanitarian Law\", \"International Criminal Court\", \"International Treaties\", \"Refugee Law\", \"UN Conventions\", \"War Crimes\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "all_subdomains = [sub for sublist in domain_structure.values() for sub in sublist]\n",
        "\n",
        "# Classify function\n",
        "def classify_subdomain(text):\n",
        "    result = classifier(text, candidate_labels=all_subdomains)\n",
        "    return result['labels'][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OALFZlu_TxH9",
        "outputId": "89d94591-f677-422a-ff01-e52793bfe308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Zero-shot classifier loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv(recommendations):\n",
        "    \"\"\"\n",
        "    Save recommended papers to CSV.\n",
        "    Now recommendations is a list of dicts (from find_related_papers) not a DataFrame.\n",
        "    \"\"\"\n",
        "    if isinstance(recommendations, list):\n",
        "        df_out = pd.DataFrame(recommendations)\n",
        "    else:\n",
        "        df_out = recommendations\n",
        "\n",
        "    # Check if similarity_score exists in recommendations\n",
        "    columns_to_save = ['title', 'clean_text', 'link', 'domain', 'subdomain']\n",
        "    if 'similarity_score' in df_out.columns:\n",
        "        columns_to_save.append('similarity_score')\n",
        "\n",
        "    df_out = df_out[columns_to_save]\n",
        "    df_out.to_csv(\"legal_case_recommendations.csv\", index=False)\n",
        "    return \"legal_case_recommendations.csv\""
      ],
      "metadata": {
        "id": "Cxnoe9Rn_c9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv_wrapper(user_input, domain, subdomain, task_choice):\n",
        "    domain_filter = domain if domain and domain.strip() != \"\" else \"All\"\n",
        "    subdomain_filter = subdomain if subdomain and subdomain.strip() != \"\" else \"All\"\n",
        "\n",
        "    recommendations = recommender_tool_instance._run(\n",
        "        user_input=user_input,\n",
        "        top_k=5,\n",
        "        domain_filter=domain_filter,\n",
        "        subdomain_filter=subdomain_filter\n",
        "    )\n",
        "\n",
        "    return generate_csv(recommendations)\n"
      ],
      "metadata": {
        "id": "V0JrmbCMXDUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input_text = \"\"\"\n",
        "In the Supreme Court of Newlandia, the petitioner, Mr. James Cooper, challenged the constitutionality of the Emergency Powers Act of 2022,\n",
        "which authorized the executive branch to detain individuals indefinitely during national crises without judicial review.\n",
        "The Act was passed in response to a series of cyberattacks that crippled national infrastructure. Mr. Cooper, a civil liberties\n",
        "advocate, was detained for publishing articles critical of the government's handling of the crisis. He contended that his detention\n",
        "violated fundamental rights guaranteed under the Newlandian Constitution, including the right to free speech and protection from arbitrary arrest.\n",
        "The government argued that such measures were necessary to safeguard national security. The central issue before the court was whether\n",
        "the Act's provisions, particularly the suspension of habeas corpus, were consistent with constitutional limits on executive power.\n",
        "\"\"\"\n",
        "\n",
        "print(\" USER INPUT TEXT\")\n",
        "print(user_input_text)\n",
        "print(\"\\n==============================\\n\")\n",
        "\n",
        "# 1. Summarize using structured legal summarizer tool\n",
        "structured_output = legal_structurer_tool_instance._run(user_input_text)\n",
        "print(\" STRUCTURED LEGAL OUTPUT\")\n",
        "print(structured_output)\n",
        "print(\"\\n==============================\\n\")\n",
        "\n",
        "# 2. Recommendations using recommender tool\n",
        "recommendations = recommender_tool_instance._run(user_input_text)\n",
        "\n",
        "print(\" RECOMMENDED CASES\")\n",
        "for rec in recommendations:\n",
        "    print(f\"- Title: {rec.get('title', 'N/A')}\")\n",
        "\n",
        "    # Domain & Subdomain\n",
        "    print(f\"  Domain: {rec.get('domain', 'N/A')}\")\n",
        "    print(f\"  Subdomain: {rec.get('subdomain', 'N/A')}\")\n",
        "\n",
        "    # Date\n",
        "    if 'date' in rec:\n",
        "        print(f\"  Date: {rec.get('date', 'N/A')}\")\n",
        "    elif 'published' in rec:\n",
        "        print(f\"  Published: {rec.get('published', 'N/A')}\")\n",
        "\n",
        "    # Link\n",
        "    if 'link' in rec:\n",
        "        print(f\"  Link: {rec.get('link', '#')}\")\n",
        "\n",
        "    # Similarity Score\n",
        "    if 'similarity_score' in rec:\n",
        "        print(f\"  Similarity Score: {rec.get('similarity_score', 0):.2f}\")\n",
        "\n",
        "    # Legal Summary or Clean Text\n",
        "    print(f\"  Summary: {rec.get('clean_text', 'No summary available')}\")\n",
        "    print(\"--------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBrOh7tAZey6",
        "outputId": "31c2573f-071d-409c-dece-b04691026f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " USER INPUT TEXT\n",
            "\n",
            "In the Supreme Court of Newlandia, the petitioner, Mr. James Cooper, challenged the constitutionality of the Emergency Powers Act of 2022,\n",
            "which authorized the executive branch to detain individuals indefinitely during national crises without judicial review.\n",
            "The Act was passed in response to a series of cyberattacks that crippled national infrastructure. Mr. Cooper, a civil liberties\n",
            "advocate, was detained for publishing articles critical of the government's handling of the crisis. He contended that his detention\n",
            "violated fundamental rights guaranteed under the Newlandian Constitution, including the right to free speech and protection from arbitrary arrest.\n",
            "The government argued that such measures were necessary to safeguard national security. The central issue before the court was whether\n",
            "the Act's provisions, particularly the suspension of habeas corpus, were consistent with constitutional limits on executive power.\n",
            "\n",
            "\n",
            "==============================\n",
            "\n",
            " STRUCTURED LEGAL OUTPUT\n",
            "The petitioner, a civil liberties advocate, challenged the constitutionality of the Emergency Powers Act of 2022, which authorized the executive branch to detain individuals indefinitely during national crises without judicial review. He alleged that his detention violated fundamental rights guaranteed under the Newlandian Constitution, including the right to free speech and protection from arbitrary arrest.\n",
            "\n",
            "==============================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3296061537.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  cos_sim = float(np.dot(user_norm, doc_norm.T))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RECOMMENDED CASES\n",
            "- Title: Roe v. Wade, 931 F.3d (2008)\n",
            "  Domain: Constitutional & Administrative Law\n",
            "  Subdomain: Emergency Powers\n",
            "  Published: 2021-08-30\n",
            "  Link: https://www.courtlistener.com/opinion/8073784/\n",
            "  Similarity Score: 0.51\n",
            "  Summary: This judgment concerns emergency powers ruling. this legal dispute explores aspects of emergency powers within the context of constitutional & administrative law, focusing on recent developments and challenges.\n",
            "--------------------------------\n",
            "- Title: Roe v. Wade, 737 U.S. (2006)\n",
            "  Domain: Constitutional & Administrative Law\n",
            "  Subdomain: Emergency Powers\n",
            "  Published: 2016-08-16\n",
            "  Link: https://www.courtlistener.com/opinion/4837518/\n",
            "  Similarity Score: 0.51\n",
            "  Summary: The opinion explores legal issues surrounding emergency powers ruling. in a significant legal interpretation, the case examines evolving standards related to emergency powers under the umbrella of constitutional & administrative law.\n",
            "--------------------------------\n",
            "- Title: Davis v. IRS, 163 S.Ct. (2012)\n",
            "  Domain: Constitutional & Administrative Law\n",
            "  Subdomain: Emergency Powers\n",
            "  Published: 2019-10-05\n",
            "  Link: https://www.courtlistener.com/opinion/9210137/\n",
            "  Similarity Score: 0.51\n",
            "  Summary: This matter involves emergency powers ruling. a dispute that highlights regulatory and constitutional questions surrounding emergency powers in relation to constitutional & administrative law.\n",
            "--------------------------------\n",
            "- Title: Garcia v. Department of Justice, 833 Fed. Appx. (2012)\n",
            "  Domain: Constitutional & Administrative Law\n",
            "  Subdomain: Emergency Powers\n",
            "  Published: 2022-12-30\n",
            "  Link: https://www.courtlistener.com/opinion/7912821/\n",
            "  Similarity Score: 0.50\n",
            "  Summary: The opinion explores legal issues surrounding emergency powers ruling. the case sets a precedent in the field of emergency powers, emphasizing judicial interpretation and statutory application under constitutional & administrative law.\n",
            "--------------------------------\n",
            "- Title: Brown v. Board of Education, 414 S.Ct. (2008)\n",
            "  Domain: Constitutional & Administrative Law\n",
            "  Subdomain: Emergency Powers\n",
            "  Published: 2016-09-12\n",
            "  Link: https://www.courtlistener.com/opinion/2442579/\n",
            "  Similarity Score: 0.50\n",
            "  Summary: The opinion explores legal issues surrounding emergency powers ruling. the case sets a precedent in the field of emergency powers, emphasizing judicial interpretation and statutory application under constitutional & administrative law.\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_apa_legal(title, published, link):\n",
        "    try:\n",
        "        year = published.split(\"-\")[0]\n",
        "    except:\n",
        "        year = \"n.d.\"\n",
        "    return f\"<i>{title}</i> ({year}).<br>Retrieved from <a href='{link}' target='_blank'>{link}</a>\"\n",
        "\n"
      ],
      "metadata": {
        "id": "CUGtGvTbBG7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "try:\n",
        "    import absl.logging\n",
        "    from unittest.mock import MagicMock\n",
        "\n",
        "    def safe_close(self):\n",
        "        try:\n",
        "            if hasattr(self.stream, 'close'):\n",
        "                self.stream.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "    absl.logging.PythonHandler.close = safe_close\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "for name in ['uvicorn', 'uvicorn.access', 'uvicorn.error', 'uvicorn.asgi', 'httpx', 'httpcore', 'asyncio', 'websockets']:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.disabled = True\n",
        "    logger.propagate = False\n",
        "    for handler in logger.handlers[:]:\n",
        "        logger.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(stream=sys.stderr, level=logging.ERROR, force=True)\n"
      ],
      "metadata": {
        "id": "9-MOMCjxcBqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def update_subdomains(domain):\n",
        "    \"\"\"Update subdomain dropdown based on selected domain.\"\"\"\n",
        "    if domain in domain_structure:\n",
        "        return gr.update(choices=[\"All\"] + domain_structure[domain], value=\"All\")\n",
        "    return gr.update(choices=[], value=None)\n",
        "\n",
        "def run_agent_ui(user_input, domain, subdomain, task):\n",
        "    if not user_input.strip():\n",
        "        return \"⚠️ Please enter a legal case text.\", \"<p style='color:red;'>No cases found.</p>\"\n",
        "\n",
        "    # filters\n",
        "    domain_filter = domain if domain and domain.strip() != \"\" else \"All\"\n",
        "    subdomain_filter = subdomain if subdomain and subdomain.strip() != \"\" else \"All\"\n",
        "\n",
        "    try:\n",
        "        # 🟢 Only Summarize (agent)\n",
        "        if task == \"Summarize\":\n",
        "            summary = run_structured_summary_agent(user_input)\n",
        "            return summary, \"📄 Only summarizer will run. Recommender not requested.\"\n",
        "\n",
        "        # 🟣 Only Recommend (agent)\n",
        "        elif task == \"Recommend Cases\":\n",
        "            summary = \"📚 Only recommender will run. Summary not requested.\"\n",
        "            recommendations = run_recommender_agent(\n",
        "                user_input=user_input,\n",
        "                domain_filter=domain_filter,\n",
        "                subdomain_filter=subdomain_filter,\n",
        "                top_k=5\n",
        "            )\n",
        "            return summary, _recs_to_html(recommendations)\n",
        "\n",
        "        # 🔵 Summarize + Recommend (agents via combined crew)\n",
        "        elif task == \"Summarize + Recommend Cases\":\n",
        "            summary_structured, recommendations = run_combined_crew(\n",
        "                user_input=user_input,\n",
        "                domain_filter=domain_filter,\n",
        "                subdomain_filter=subdomain_filter,\n",
        "                top_k=5\n",
        "            )\n",
        "            return summary_structured, _recs_to_html(recommendations)\n",
        "\n",
        "        # Fallback\n",
        "        return \"Unknown task.\", \"<p>No output.</p>\"\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"❌ Error processing request: {e}\"\n",
        "        return msg, msg\n",
        "\n",
        "# Build the Gradio Interface\n",
        "with gr.Blocks(title=\"🧠 Legal Summarizer + Recommender Agent\") as demo:\n",
        "    gr.Markdown(\"## 🧠 Legal Summarizer + Recommender Agent\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            user_input = gr.Textbox(label=\"Input Legal Case Text\", lines=8, placeholder=\"Enter full legal case text, issue, facts, or ruling here..\")\n",
        "            domain = gr.Dropdown(choices=[\"All\"] + list(domain_structure.keys()), label=\"Domain (Optional)\", value=None)\n",
        "            subdomain = gr.Dropdown(choices=[], label=\"Subdomain (Optional)\", value=None)\n",
        "\n",
        "            # Dynamic update of subdomains\n",
        "            domain.change(fn=update_subdomains, inputs=domain, outputs=subdomain)\n",
        "\n",
        "            task_choice = gr.Dropdown(\n",
        "              choices=[\"Summarize\", \"Recommend Cases\", \"Summarize + Recommend Cases\"],\n",
        "              label=\"Select Task\",\n",
        "              value=None\n",
        "            )\n",
        "\n",
        "            submit_btn = gr.Button(\"▶️ Start Agent\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            summary_output = gr.Textbox(label=\"Structured Summary of Legal Case\", lines=6)\n",
        "            recommendations_output = gr.HTML(label=\"Recommended Legal Cases\")\n",
        "            download_btn = gr.Button(\"📥 Download Recommended Legal Cases as CSV\")\n",
        "\n",
        "\n",
        "    # Connect button\n",
        "    submit_btn.click(fn=run_agent_ui,\n",
        "                     inputs=[user_input, domain, subdomain, task_choice],\n",
        "                     outputs=[summary_output, recommendations_output])\n",
        "\n",
        "    download_btn.click(\n",
        "      fn=generate_csv_wrapper,\n",
        "      inputs=[user_input, domain, subdomain, task_choice],\n",
        "      outputs=gr.File(label=\"Download Recommendations CSV\")\n",
        "    )\n",
        "\n",
        "\n",
        "def run_gradio_safe():\n",
        "    print(\"🚀 Starting Gradio safely for Colab...\")\n",
        "    try:\n",
        "        import os\n",
        "        os.environ[\"GRADIO_SERVER_NAME\"] = \"0.0.0.0\"\n",
        "        logging.getLogger().handlers.clear()\n",
        "        demo.launch(share=True, show_error=True)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gradio launch failed: {e}\")\n",
        "run_gradio_safe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "2OawY84qo5lq",
        "outputId": "52f18a39-0455-4595-8c43-d1694eecaf6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Gradio safely for Colab...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cbdfa761c323409517.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cbdfa761c323409517.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                **END OF CODE.....EXTRA AND COPY OF CODES BELOW. IGNORE**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD7tNV_PLzW6"
      }
    }
  ]
}